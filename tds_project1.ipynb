{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# Set the API token directly\n",
        "api_token = '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'  # Replace with your actual token\n",
        "\n",
        "# GitHub API URL\n",
        "base_url = 'https://api.github.com'\n",
        "headers = {'Authorization': f'token {api_token}'}\n",
        "\n",
        "# Track start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Rate limit checker\n",
        "def check_rate_limit():\n",
        "    rate_url = f\"{base_url}/rate_limit\"\n",
        "    response = requests.get(rate_url, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "users_data = []\n",
        "page = 1\n",
        "\n",
        "while True:\n",
        "    # Check rate limit\n",
        "    rate_limit_info = check_rate_limit()\n",
        "    remaining_requests = rate_limit_info['rate']['remaining']\n",
        "    reset_time = rate_limit_info['rate']['reset']\n",
        "\n",
        "    if remaining_requests < 1:\n",
        "        wait_time = reset_time - int(time.time()) + 1\n",
        "        print(f\"Rate limit exceeded. Waiting for {wait_time} seconds.\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "    # Fetch users\n",
        "    users_url = f\"{base_url}/search/users?q=location:Sydney+followers:>100&page={page}&per_page=100\"\n",
        "    response = requests.get(users_url, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: {response.status_code} - {response.text}\")\n",
        "        break\n",
        "\n",
        "    data = response.json()\n",
        "\n",
        "    if 'items' not in data or not data['items']:\n",
        "        break\n",
        "\n",
        "    users_data.extend(data['items'])\n",
        "    page += 1\n",
        "\n",
        "# Extract user info\n",
        "users = []\n",
        "for user in users_data:\n",
        "    user_detail_url = user['url']\n",
        "    user_response = requests.get(user_detail_url, headers=headers)\n",
        "    user_info = user_response.json()\n",
        "\n",
        "    # Clean up company name\n",
        "    company = user_info.get('company', '')\n",
        "    if company:\n",
        "        company = company.strip(' ').lstrip('@').upper()\n",
        "\n",
        "    users.append({\n",
        "        'login': user_info['login'],\n",
        "        'name': user_info['name'],\n",
        "        'company': company,\n",
        "        'location': user_info['location'],\n",
        "        'email': user_info['email'],\n",
        "        'hireable': 'true' if user_info['hireable'] else 'false',\n",
        "        'bio': user_info['bio'],\n",
        "        'public_repos': user_info['public_repos'],\n",
        "        'followers': user_info['followers'],\n",
        "        'following': user_info['following'],\n",
        "        'created_at': user_info['created_at']\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame and save as CSV\n",
        "users_df = pd.DataFrame(users)\n",
        "users_csv_path = 'users.csv'\n",
        "users_df.to_csv(users_csv_path, index=False)\n",
        "\n",
        "# Fetch repositories for each user\n",
        "repos = []\n",
        "for user in users:\n",
        "    page = 1\n",
        "    user_repos = []\n",
        "    while True:\n",
        "        repos_url = f\"{base_url}/users/{user['login']}/repos?sort=pushed&direction=desc&page={page}&per_page=100\"\n",
        "        repos_response = requests.get(repos_url, headers=headers)\n",
        "        repos_data = repos_response.json()\n",
        "\n",
        "        if not repos_data or len(user_repos) >= 500:\n",
        "            break\n",
        "\n",
        "        for repo in repos_data:\n",
        "            if len(user_repos) >= 500:\n",
        "                break\n",
        "\n",
        "            user_repos.append({\n",
        "                'login': user['login'],\n",
        "                'full_name': repo['full_name'],\n",
        "                'created_at': repo['created_at'],\n",
        "                'stargazers_count': repo['stargazers_count'],\n",
        "                'watchers_count': repo['watchers_count'],\n",
        "                'language': repo['language'],\n",
        "                'has_projects': 'true' if repo['has_projects'] else 'false',\n",
        "                'has_wiki': 'true' if repo['has_wiki'] else 'false',\n",
        "                'license_name': repo['license']['name'] if repo['license'] else None\n",
        "            })\n",
        "\n",
        "        page += 1\n",
        "    repos.extend(user_repos)\n",
        "\n",
        "# Convert to DataFrame and save as CSV\n",
        "repos_df = pd.DataFrame(repos)\n",
        "repos_csv_path = 'repositories.csv'\n",
        "repos_df.to_csv(repos_csv_path, index=False)\n",
        "\n",
        "# Track end time\n",
        "end_time = time.time()\n",
        "execution_time = str(datetime.timedelta(seconds=(end_time - start_time)))\n",
        "\n",
        "print(f\"Data scraping and file creation completed in {execution_time}.\")\n",
        "print(\"Users.csv and repositories.csv are available for download.\")\n",
        "\n",
        "# Provide download options\n",
        "from IPython.display import FileLink\n",
        "\n",
        "display(FileLink(users_csv_path, result_html_prefix=\"Download users.csv: \"))\n",
        "display(FileLink(repos_csv_path, result_html_prefix=\"Download repositories.csv: \"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "rjMgyv1HY1uy",
        "outputId": "44a576be-d17c-4ead-e5f9-510c995de109"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scraping and file creation completed in 0:09:08.174934.\n",
            "Users.csv and repositories.csv are available for download.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/content/users.csv"
            ],
            "text/html": [
              "Download users.csv: <a href='users.csv' target='_blank'>users.csv</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/content/repositories.csv"
            ],
            "text/html": [
              "Download repositories.csv: <a href='repositories.csv' target='_blank'>repositories.csv</a><br>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8TRXpIkYmse",
        "outputId": "b6d54571-1fcd-4b1e-ddbd-030e7bc00bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Top 5 users by followers: nicknochnack,brendangregg,cornflourblue,0vm,davecheney\n",
            "2. 5 earliest registered users: dylanegan,cjheath,freshtonic,dhowden,mikel\n",
            "3. 3 most popular licenses: MIT License,Other,Apache License 2.0\n",
            "4. Majority company: ATLASSIAN\n",
            "5. Most popular programming language: JavaScript\n",
            "6. Second most popular language among users who joined after 2020: TypeScript\n",
            "7. Language with highest average stars per repository: Mermaid\n",
            "8. Top 5 users by leader strength: brendangregg,cornflourblue,Canva,nicknochnack,0vm\n",
            "9. Correlation between followers and public repos: 0.035\n",
            "10. Regression slope of followers on repos: 0.068\n",
            "11. Correlation between projects and wiki: 0.251\n",
            "12. Average following difference for hireable users: 54.408\n",
            "13. Regression slope of followers on bio word count: -10.946\n",
            "14. Top 5 users creating most repos on weekends: timgates42,pinkforest,johndpope,mvandermeulen,mikeyhodl\n",
            "15. Email sharing difference for hireable users: 0.051\n",
            "16. Most common surname(s): Wu,Zhang\n",
            "Number of users with the most common surname: 4\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load the CSV files\n",
        "users_df = pd.read_csv('users.csv')\n",
        "repositories_df = pd.read_csv('repositories.csv')\n",
        "\n",
        "# Ensure 'created_at' in users_df is in datetime format\n",
        "users_df['created_at'] = pd.to_datetime(users_df['created_at'], errors='coerce')\n",
        "\n",
        "# Question 1: Top 5 users in Sydney with the highest number of followers\n",
        "top_5_followers = users_df.nlargest(5, 'followers')['login'].tolist()\n",
        "print(\"1. Top 5 users by followers:\", ','.join(top_5_followers))\n",
        "\n",
        "# Question 2: 5 earliest registered GitHub users in Sydney\n",
        "earliest_5_users = users_df.nsmallest(5, 'created_at')['login'].tolist()\n",
        "print(\"2. 5 earliest registered users:\", ','.join(earliest_5_users))\n",
        "\n",
        "# Question 3: 3 most popular licenses among these users\n",
        "popular_licenses = repositories_df['license_name'].value_counts().nlargest(3).index.tolist()\n",
        "print(\"3. 3 most popular licenses:\", ','.join(popular_licenses))\n",
        "\n",
        "# Question 4: Company with the majority of developers\n",
        "majority_company = users_df['company'].str.strip().str.upper().mode()[0]\n",
        "print(\"4. Majority company:\", majority_company)\n",
        "\n",
        "# Question 5: Most popular programming language among users\n",
        "popular_language = repositories_df['language'].mode()[0]  # Make sure to check the correct column name here\n",
        "print(\"5. Most popular programming language:\", popular_language)\n",
        "\n",
        "# Question 6: Second most popular programming language among users who joined after 2020\n",
        "recent_users_repos = repositories_df[repositories_df['login'].isin(users_df[users_df['created_at'] > '2020-01-01']['login'])]\n",
        "second_popular_language = recent_users_repos['language'].value_counts().nlargest(2).index[-1]\n",
        "print(\"6. Second most popular language among users who joined after 2020:\", second_popular_language)\n",
        "\n",
        "# Question 7: Language with the highest average number of stars per repository\n",
        "avg_stars_per_language = repositories_df.groupby('language')['stargazers_count'].mean().idxmax()  # Make sure to check the correct column name here\n",
        "print(\"7. Language with highest average stars per repository:\", avg_stars_per_language)\n",
        "\n",
        "# Question 8: Top 5 users by leader_strength\n",
        "users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])\n",
        "top_5_leader_strength = users_df.nlargest(5, 'leader_strength')['login'].tolist()\n",
        "print(\"8. Top 5 users by leader strength:\", ','.join(top_5_leader_strength))\n",
        "\n",
        "# Question 9: Correlation between followers and public repositories\n",
        "correlation_followers_repos = users_df['followers'].corr(users_df['public_repos'])\n",
        "print(f\"9. Correlation between followers and public repos: {correlation_followers_repos:.3f}\")\n",
        "\n",
        "# Question 10: Regression slope of followers on repos\n",
        "X = users_df['public_repos'].values.reshape(-1, 1)\n",
        "y = users_df['followers'].values\n",
        "model = LinearRegression().fit(X, y)\n",
        "regression_slope = model.coef_[0]\n",
        "print(f\"10. Regression slope of followers on repos: {regression_slope:.3f}\")\n",
        "\n",
        "# Question 11: Correlation between projects and wiki enabled\n",
        "projects_wiki_correlation = repositories_df['has_projects'].corr(repositories_df['has_wiki'])\n",
        "print(f\"11. Correlation between projects and wiki: {projects_wiki_correlation:.3f}\")\n",
        "\n",
        "# Question 12: Average of following per user for hireable\n",
        "hireable_avg_following = users_df[users_df['hireable'] == True]['following'].mean()\n",
        "non_hireable_avg_following = users_df[users_df['hireable'] == False]['following'].mean()\n",
        "following_difference = hireable_avg_following - non_hireable_avg_following\n",
        "print(f\"12. Average following difference for hireable users: {following_difference:.3f}\")\n",
        "\n",
        "# Question 13: Regression slope of followers on bio word count\n",
        "users_with_bio = users_df.dropna(subset=['bio']).copy()\n",
        "users_with_bio['bio_word_count'] = users_with_bio['bio'].apply(lambda x: len(x.split()))\n",
        "X = users_with_bio['bio_word_count']\n",
        "y = users_with_bio['followers']\n",
        "X = sm.add_constant(X)\n",
        "bio_model = sm.OLS(y, X).fit()\n",
        "bio_slope = bio_model.params['bio_word_count']\n",
        "print(f\"13. Regression slope of followers on bio word count: {bio_slope:.3f}\")\n",
        "\n",
        "# Question 14: Top 5 users who created the most repositories on weekends (UTC)\n",
        "repositories_df['created_at'] = pd.to_datetime(repositories_df['created_at'])\n",
        "repositories_df['is_weekend'] = repositories_df['created_at'].dt.dayofweek >= 5\n",
        "weekend_repos_count = repositories_df[repositories_df['is_weekend']].groupby('login').size()\n",
        "top_5_weekend_users = weekend_repos_count.nlargest(5).index.tolist()\n",
        "print(\"14. Top 5 users creating most repos on weekends:\", ','.join(top_5_weekend_users))\n",
        "\n",
        "# Question 15: Hireable users sharing their email more often\n",
        "hireable_with_email = users_df[users_df['hireable'] == True]['email'].notna().mean()\n",
        "non_hireable_with_email = users_df[users_df['hireable'] == False]['email'].notna().mean()\n",
        "email_difference = hireable_with_email - non_hireable_with_email\n",
        "print(f\"15. Email sharing difference for hireable users: {email_difference:.3f}\")\n",
        "\n",
        "# Question 16: Most common surname(s)\n",
        "users_df['surname'] = users_df['name'].apply(lambda x: str(x).strip().split()[-1].capitalize() if isinstance(x, str) and x.strip() else None)\n",
        "surname_counts = users_df['surname'].value_counts().dropna()\n",
        "max_surname_count = surname_counts.max()\n",
        "most_common_surnames = surname_counts[surname_counts == max_surname_count].index.tolist()\n",
        "most_common_surnames.sort()\n",
        "print(\"16. Most common surname(s):\", ','.join(most_common_surnames))\n",
        "print(\"Number of users with the most common surname:\", max_surname_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the users.csv file\n",
        "files.download('users.csv')\n",
        "\n",
        "# Download the repositories.csv file\n",
        "files.download('repositories.csv')"
      ],
      "metadata": {
        "id": "oFVaFoTtc2m8",
        "outputId": "42e887cf-f16c-4bad-fb79-95caabfd1969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ff1623c7-19c0-4095-b97e-6b5074a7858c\", \"users.csv\", 53225)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5f86573f-16a9-48a2-9a3d-8f0efdff6462\", \"repositories.csv\", 2823187)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}